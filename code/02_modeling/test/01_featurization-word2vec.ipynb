{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure and import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show azureml-tatk version\n",
    "!pip show azureml-tatk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\remoteuser\\AppData\\Local\\AmlWorkbench\\Python\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tatk.utils import load_biomedical_data, download_embedding_model, data_dir, dictionaries_dir, models_dir\n",
    "from tatk.connectors.blob_storage_data_connector import AzureBlobStorageDataConnector\n",
    "from tatk.pipelines.feature_extraction.word2vec_model import Word2VecModel\n",
    "from tatk.feature_extraction.word2vec_vectorizer import Word2VecVectorizer\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  \n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data processing and batch preparation\n",
    "# In the following code, we replace Emails, URLS, emoticons etc with special labels\n",
    "\n",
    "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
    "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Ã\",\":-Ã\",\":X\",\n",
    "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
    "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
    "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"Ê˜â€¿Ê˜\",\"â¤\",\"ğŸ’œ\",\"ğŸ’š\",\"ğŸ’•\",\"ğŸ’™\",\"ğŸ’›\",\"ğŸ’“\",\"ğŸ’\",\"ğŸ’–\",\"ğŸ’\",\n",
    "               \"ğŸ’˜\",\"ğŸ’—\",\"ğŸ˜—\",\"ğŸ˜˜\",\"ğŸ˜™\",\"ğŸ˜š\",\"ğŸ˜»\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜ƒ\",\"â˜º\",\"ğŸ˜„\",\"ğŸ˜†\",\"ğŸ˜‡\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜\",\n",
    "               \"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜®\",\"ğŸ˜¸\",\"ğŸ˜¹\",\"ğŸ˜º\",\"ğŸ˜»\",\"ğŸ˜¼\",\"ğŸ‘\"]\n",
    "\n",
    "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(Â¬_Â¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
    "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
    "               \"`_Â´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"ğŸ’”\",\"â˜¹ï¸\",\"ğŸ˜Œ\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜”\",\"ğŸ˜•\",\"ğŸ˜–\",\"ğŸ˜\",\"ğŸ˜Ÿ\",\n",
    "               \"ğŸ˜ \",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜£\",\"ğŸ˜¤\",\"ğŸ˜¥\",\"ğŸ˜¦\",\"ğŸ˜§\",\"ğŸ˜¨\",\"ğŸ˜©\",\"ğŸ˜ª\",\"ğŸ˜«\",\"ğŸ˜¬\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜°\",\"ğŸ˜±\",\"ğŸ˜²\",\n",
    "               \"ğŸ˜³\",\"ğŸ˜´\",\"ğŸ˜·\",\"ğŸ˜¾\",\"ğŸ˜¿\",\"ğŸ™€\",\"ğŸ’€\",\"ğŸ‘\"]\n",
    "\n",
    "# Emails\n",
    "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "\n",
    "# Mentions\n",
    "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
    "\n",
    "#Urls\n",
    "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
    "\n",
    "#Numerics\n",
    "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
    "\n",
    "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
    "\n",
    "emoticonsDict = {}\n",
    "for i,each in enumerate(pos_emoticons):\n",
    "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "for i,each in enumerate(neg_emoticons):\n",
    "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
    "    \n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
    "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tweets(filename):\n",
    "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        all_lines=f.readlines()\n",
    "        padded_lines=[]\n",
    "        for line in all_lines:\n",
    "            line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
    "            line = userMentionsRegex.sub(' USER ', line )\n",
    "            line = emailsRegex.sub(' EMAIL ', line )\n",
    "            line=urlsRegex.sub(' URL ', line)\n",
    "            line=numsRegex.sub(' NUM ',line)\n",
    "            line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
    "            line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
    "            words_tokens=[token for token in TweetTokenizer().tokenize(line)]                    \n",
    "            line= ' '.join(token for token in words_tokens )         \n",
    "            padded_lines.append(line)\n",
    "    return padded_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>damn fixtated on USER lovely thighs PUN hips o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>god bless firefox PUN s ' restore previous ses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER http://twitpic PUN com PUN 6vn4a - dang g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USER hey PUN sorry u had to come back to work ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bye mommy PUN we PUN ll miss you PUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets\n",
       "0  damn fixtated on USER lovely thighs PUN hips o...\n",
       "1  god bless firefox PUN s ' restore previous ses...\n",
       "2  USER http://twitpic PUN com PUN 6vn4a - dang g...\n",
       "3  USER hey PUN sorry u had to come back to work ...\n",
       "4               bye mommy PUN we PUN ll miss you PUN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_filename = r\"D:\\Sentiment140_Classification\\training_text.csv\"\n",
    "tweets = read_tweets(training_filename)\n",
    "\n",
    "df = pd.DataFrame({'tweets':tweets})\n",
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::create_pipeline ==> start\n",
      "input_col=tweets\n",
      "input_col=NltkPreprocessor1f41071076a64f93b082fd888a6015e7\n",
      "input_col=UngroupTransformer5d9b2013721e45dea4a222afc1cd23c6\n",
      ":: number of jobs for the pipeline : 6\n",
      "0\tnltk_preprocessor\n",
      "1\tungroup_transformer\n",
      "Word2VecModel::create_pipeline ==> end\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = Word2VecModel(input_col = 'tweets', regex = None, detect_sentences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tatk.pipelines.feature_extraction.word2vec_model.Word2VecModel"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel TATK Pipeline:\n",
      "0 - nltk_preprocessor(tweets,NltkPreprocessorcdf2b441e3ec469da0dfc5e0e447bd42)\n",
      "1 - ungroup_transformer(NltkPreprocessorcdf2b441e3ec469da0dfc5e0e447bd42,UngroupTransformer3832ece195f0418b921b5c0988877d3b)\n",
      "2 - vectorizer(UngroupTransformer3832ece195f0418b921b5c0988877d3b,Word2VecVectorizer0a03e9fbaea945fb99b3482cedf830e2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2) Display and Change default pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec_model.get_step_id(nltk_preprocessor)\n",
      "0\n",
      "word2vec_model.get_step_id(vectorizer)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Get step indices. \n",
    "print(\"word2vec_model.get_step_id({})\".format(\"nltk_preprocessor\"))\n",
    "print(word2vec_model.get_step_id(\"nltk_preprocessor\"))\n",
    "\n",
    "print(\"word2vec_model.get_step_id({})\".format(\"vectorizer\"))\n",
    "print(word2vec_model.get_step_id(\"vectorizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aggregation_func': <function tatk.feature_extraction.word2vec_vectorizer.Word2VecVectorizer.aggregate_mean>,\n",
       " 'case_sensitive': False,\n",
       " 'context_window_size': 5,\n",
       " 'copy_from_path': True,\n",
       " 'embedding_size': 100,\n",
       " 'embedding_table': None,\n",
       " 'get_from_path': True,\n",
       " 'input_col': 'UngroupTransformer3832ece195f0418b921b5c0988877d3b',\n",
       " 'min_df': 5,\n",
       " 'negative_sample_size': 5,\n",
       " 'num_epochs': 5,\n",
       " 'num_workers': 4,\n",
       " 'output_col': 'Word2VecVectorizer0a03e9fbaea945fb99b3482cedf830e2',\n",
       " 'return_type': 'word_vector',\n",
       " 'save_overwrite': True,\n",
       " 'skip_OOV': False,\n",
       " 'trainable': True,\n",
       " 'trained_model': None,\n",
       " 'use_hierarchical_softmax': 0,\n",
       " 'use_skipgram': 0}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.get_step_params_by_name('vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change minimum and maximum subwords size.  \n",
    "#word2vec_model.set_step_params_by_name('vectorizer', min_char_ngrams = 4, max_char_ngrams = 5) \n",
    "#word2vec_model.get_step_params_by_name('vectorizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.3) Fit the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecModel::fit ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> start\n",
      "NltkPreprocessor::tatk_fit_transform ==> end \t Time taken: 1.52 mins\n",
      "UngroupTransformer::tatk_fit_transform ==> start\n",
      "UngroupTransformer::tatk_fit_transform ==> end \t Time taken: 0.03 mins\n",
      "Word2VecVectorizer::tatk_fit ==> start\n",
      "vocabulary size =49252\n",
      "Word2VecVectorizer::tatk_fit ==> end \t Time taken: 1.16 mins\n",
      "Time taken: 2.71 mins\n",
      "Word2VecModel::fit ==> end\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2VecModel(detect_sentences=True, input_col='tweets', regex=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save and Load pipeline for additional training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Save and Load the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTextModel::save ==> start\n",
      "TatkPipeline::save ==> start\n",
      "Time taken: 0.01 mins\n",
      "TatkPipeline::save ==> end\n",
      "Time taken: 0.01 mins\n",
      "BaseTextModel::save ==> end\n",
      "BaseTextModel::load ==> start\n",
      "TatkPipeline::load ==> start\n",
      "Word2VecVectorizer: Word2Vec model loaded from D:\\Sentiment140_Classification\\word2vec_model\\pipeline\\vectorizer\\embedding_model.gen\n",
      "Time taken: 0.01 mins\n",
      "TatkPipeline::load ==> end\n",
      "Time taken: 0.01 mins\n",
      "BaseTextModel::load ==> end\n"
     ]
    }
   ],
   "source": [
    "models_dir = r'D:\\Sentiment140_Classification'\n",
    "pipeline_path = os.path.join(models_dir, 'word2vec_model')\n",
    "word2vec_model.save(pipeline_path, create_folders_on_path=True)\n",
    "word2vec_model2 = Word2VecModel.load(pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Perform additional training on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save and Load Embeddings For Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.1) Save the embeddings from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::save_embeddings ==> start\n",
      "Time taken: 0.05 mins\n",
      "Word2VecVectorizer::save_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "# Saved embeddings file is in textual format and is readable if opened with a text editor\n",
    "embeddings_file_path = os.path.join(models_dir, 'word2vec_embeddings.txt')\n",
    "word2vec_model2.save_embeddings(embeddings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.2) Load the embeddings to memory with include_unk set to True to add OOV treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::load_embeddings ==> start\n",
      "Time taken: 0.11 mins\n",
      "Word2VecVectorizer::load_embeddings ==> end\n"
     ]
    }
   ],
   "source": [
    "#embeddings_file_path = r'D:\\Sentiment140_Classification'\n",
    "vectorizer = Word2VecVectorizer.load_embeddings(embeddings_file_path, include_unk = True, unk_method = 'rnd', unk_vector = None, unk_word = '<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.3) Embedding Lookup: Get word and subword indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[1, 24, 1188]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[9, 1215, 15983, 21, 49252]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                      indices\n",
       "0                        I have fever                [1, 24, 1188]\n",
       "1  My doctor prescribed me ibuprofen.  [9, 1215, 15983, 21, 49252]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_predict = pd.DataFrame({'text' : [\"I have fever\", \"My doctor prescribed me ibuprofen.\"]})\n",
    "vectorizer.input_col = 'text'\n",
    "vectorizer.output_col = 'indices'\n",
    "vectorizer.return_type = 'word_index'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.4) Embedding Lookup: Get word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[116, 55, 1739]</td>\n",
       "      <td>[[-0.743229985237, -0.0243050009012, -0.760702...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[10065, 5543, 8558, 5103, 25585]</td>\n",
       "      <td>[[-0.0086089996621, -0.321725994349, 0.0310159...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                           indices  \\\n",
       "0                        I have fever                   [116, 55, 1739]   \n",
       "1  My doctor prescribed me ibuprofen.  [10065, 5543, 8558, 5103, 25585]   \n",
       "\n",
       "                                         word_vector  \n",
       "0  [[-0.743229985237, -0.0243050009012, -0.760702...  \n",
       "1  [[-0.0086089996621, -0.321725994349, 0.0310159...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'word_vector'\n",
    "vectorizer.return_type = 'word_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.5) Embedding Lookup: Get sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2VecVectorizer::tatk_transform ==> start\n",
      "Word2VecVectorizer::tatk_transform ==> end \t Time taken: 0.0 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>indices</th>\n",
       "      <th>word_vector</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have fever</td>\n",
       "      <td>[116, 55, 1739]</td>\n",
       "      <td>[[-0.743229985237, -0.0243050009012, -0.760702...</td>\n",
       "      <td>[-0.723917007446, 0.354095672568, -1.114667336...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My doctor prescribed me ibuprofen.</td>\n",
       "      <td>[10065, 5543, 8558, 5103, 25585]</td>\n",
       "      <td>[[-0.0086089996621, -0.321725994349, 0.0310159...</td>\n",
       "      <td>[0.0336762743894, -0.0964680787281, -0.1395916...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                           indices  \\\n",
       "0                        I have fever                   [116, 55, 1739]   \n",
       "1  My doctor prescribed me ibuprofen.  [10065, 5543, 8558, 5103, 25585]   \n",
       "\n",
       "                                         word_vector  \\\n",
       "0  [[-0.743229985237, -0.0243050009012, -0.760702...   \n",
       "1  [[-0.0086089996621, -0.321725994349, 0.0310159...   \n",
       "\n",
       "                                     sentence_vector  \n",
       "0  [-0.723917007446, 0.354095672568, -1.114667336...  \n",
       "1  [0.0336762743894, -0.0964680787281, -0.1395916...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.output_col = 'sentence_vector'\n",
    "vectorizer.return_type = 'sentence_vector'\n",
    "result = vectorizer.tatk_transform(df_predict)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5.6) Embedding Lookup: Get most similar word to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7867355346679688),\n",
       " ('tough', 0.7338866591453552),\n",
       " ('rough', 0.7106154561042786),\n",
       " ('nice', 0.6891409158706665),\n",
       " ('bad', 0.6804108619689941),\n",
       " ('fantastic', 0.658706784248352),\n",
       " ('fabulous', 0.6579225659370422),\n",
       " ('successful', 0.6407154202461243),\n",
       " ('terrible', 0.6375681161880493),\n",
       " ('fab', 0.6368352174758911)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embedding_table.most_similar('fever')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML Python 3.5",
   "language": "python",
   "name": "aml-python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
